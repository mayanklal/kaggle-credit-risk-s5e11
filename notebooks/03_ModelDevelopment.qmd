---
title: "Credit Risk Scorecard Model - Model Development"
author: "Mayank Lal"
date: today
format: 
    html:
        code-fold: true
        code-summary: "Code"
        code-overflow: wrap
execute:
  echo: false
  include: false
  warning: false
---
```{r setup}
## Exploratory Data Analysis
library(tidyverse) # Data Manipulation and Visualizaton
library(data.table) # Large datasets
library(vtable) # Formatted Summary Table
library(corrplot) # Correlation
library(patchwork) # Grid of plots
library(scales) # For Percentage Labels
library(rpart) # Simple Decision Tree for interaction analysis
library(rpart.plot) # Plotting Decision Trees

## Variable Selection and Feature Engineering
library(scorecard) # Information Value
library(pROC) # Gini
library(gt) # Formattting Table for publishing
library(knitr) # For simple formatting
library(DT) # For interactive tables
library(stringr) # Selecting sub-strings
library(Hmisc) # Variabel Clustering
library(car) # VIF
library(glmnet) # LASSO

# Model Development
library(MASS) # Stepwise Regression
library(lmtest) # Joint Wald Test
library(Matrix) # Create Matrix objects for XGBoost
library(xgboost) # XGBoost
library(caret) # Stratified Splitting
#library(SHAPforxgboost) # Interpretability

# Tidy Modeling
library(tidymodels)
library(recipes)
library(moments)
library(rlang)
library(vip)
library(future)
library(future.apply)
```

This is a rough sketch notebook. We will furnish it before publishing.
```{r setup}
set.seed(123)
```

```{r data}
# Load Model Data for Logistic Regression
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_final_lr.Rdata")

# Load Test Data for Logistic Regression
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_final_lr.Rdata")

# Load Model Data for Tree-based Models
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_tree.Rdata")

# Load Test Data for Tree-based Models
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_tree.Rdata")
```

## Model Development

Let's start with model development. First, we will use stepwise logistic regression to select final list of variables that improves AIC.

### Logistic Regression - Stepwise Regression (Iteration 0)

```{r lr_step}
# Create list of variables
vars <- c("working_flag_woe",
          "debt_to_income_ratio_woe",
          "credit_score_woe",
          "interest_rate_woe",
          "education_level_woe",
          "loan_purpose_woe",
          "annual_income_woe")

# Full model formula
full_formula <- as.formula(
  paste("loan_paid_back ~", paste(vars, collapse = " + "))
)

# Null model (only intercept)
null_formula <- loan_paid_back ~ 1

stepwise_model <- stepAIC(
    glm(loan_paid_back ~ ., data = train_final_lr, family = binomial),
    scope=list(lower=null_formula, upper=full_formula),
    direction="both",
    trace=TRUE
)

summary(stepwise_model)
```

### Logistic Regression - Final Model (Iteration 1)

```{r lr_final}
# Create list of final variables after stepwise regression
final_model_vars <- names(coef(stepwise_model))[-1]

# Final model formula
final_formula <- as.formula(
  paste("loan_paid_back ~", paste(final_model_vars, collapse = " + "))
)

# Build Final Logistic Regression Model
final_model <- glm(final_formula, data = train_final_lr, family = binomial)
summary(final_model)
```


#### Perform wald test for individual predictors

```{r wald_indv}
Anova(final_model)
```

### Logistic Regression - Final Model with interactions (Iteration 2)
```{r lr_interaction}
# Create interaction terms
interaction_terms <- c("debt_to_income_ratio_woe:credit_score_woe", "credit_score_woe:interest_rate_woe")

# Create final formula with interactions
final_formula_interaction <- as.formula(
  paste("loan_paid_back ~",
        paste(final_model_vars, collapse = " + "),
        "+",
        paste(interaction_terms, collapse = " + "))
)

# Build final model with interactions
final_model_interaction <- glm(final_formula_interaction,
                               data=train_final_lr,
                               family=binomial)
summary(final_model_interaction)
```

#### Perform wald joint test

```{r wald_joint}
waldtest(final_model, final_model_interaction)
```

### XGBoost - Base Model using k-fold cross-validation (Iteration 1)

In base model, we will be adding all variables in their raw form without any feature engineering.

Let's see the AUC of this base model. 

```{r xgb_cv_iter01}
# Create train and validation split 
train_idx <- createDataPartition(train_tree$loan_paid_back, p = 0.8, list = FALSE)
train_xgb <- train_tree[train_idx]
valid_xgb <- train_tree[-train_idx]

# Full formula 
feature_formula <- ~ . - id - loan_paid_back - 1

# Convert to DMatrix 
train_xgb_matrix <- model.matrix(feature_formula, data = train_xgb)
valid_xgb_matrix <- model.matrix(feature_formula, data = valid_xgb)

dtrain <- xgb.DMatrix(train_xgb_matrix, label = train_xgb$loan_paid_back)
dvalid <- xgb.DMatrix(valid_xgb_matrix, label = valid_xgb$loan_paid_back)

# Estimate Weight 
neg <- sum(train_xgb$loan_paid_back == 0)
pos <- sum(train_xgb$loan_paid_back == 1)
scale_pos_weight <- neg/pos

# Baseline parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.03,
  max_depth = 5,
  min_child_weight = 30,
  subsample = 0.8,
  colsample_bytree = 0.6,
  gamma = 1,
  lambda = 2,
  scale_pos_weight = scale_pos_weight
)

# Perform k-fold Cross-validation
xgb_cv <- xgb.cv(
  data = dtrain,
  param = params,
  nrounds = 5000,
  nfold = 10,
  early_stopping_rounds = 50,
  maximize = TRUE,
  verbose = 1
)

best_nrounds <- xgb_cv$early_stop$best_iteration

# Train Base Model
xgb_base_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  evals = list(train=dtrain, valid=dvalid),
  early_stopping_rounds = 50
)

# Predict on test dataset
feature_formula_test <- ~ . - id - 1
test_xgb_matrix <- model.matrix(feature_formula_test, data = test_tree)
test_pred_xgb  <- predict(xgb_base_model, test_xgb_matrix)
submission_xgb_iter01 <- data.frame(
  id = test_tree$id,
  loan_paid_back = test_pred_xgb
)

# Save to csv
write.csv(submission_xgb_iter01, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_iter01.csv", row.names = FALSE)

# Feature Importance 
importance_matrix <- xgb.importance(
  model = xgb_base_model,
  feature_names = colnames(dtrain)
)

xgb.plot.importance(importance_matrix)
```

### XGBoost - Tidymodels (Iteration 2)

Based on discussions on Kaggle, I realized that my model development workflow is heavily flawed. <Will elaborate on why is it flawed later on.> To bridge the gap between my workflow and my peers' workflow, I will now use tidymodels based workflow. <Will elaborte on what is tidymodels and how it helps later on.>

```{r xgb_cv_iter02}
# Prepare datasets
vb_df <- train_tree %>% dplyr::select(-id)
vb_df$loan_paid_back <- factor(vb_df$loan_paid_back, levels = c(0, 1))
vb_split <- initial_split(vb_df, strata = loan_paid_back)
vb_train <- training(vb_split)
vb_test <- testing(vb_split)

# Specify paramters of XGBoost Model that needs to be tuned
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(), min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost", eval_metric = "auc", device = "cuda") %>%
  set_mode("classification")

xgb_spec

# Specify hyperparameter space to find values for hyperparameters
xgb_grid <- grid_space_filling(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), vb_train),
  learn_rate(),
  size = 30
)

xgb_grid

# Specify Workflow
xgb_rec <- recipe(loan_paid_back ~ ., data = vb_train) %>%
  step_zv(all_predictors()) %>%
  step_dummy(all_nominal_predictors())

xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(xgb_rec)


xgb_wf

# Specify folds for k-fold cross-validation
set.seed(123)
vb_folds <- vfold_cv(vb_train, strata = loan_paid_back, v = 3)

vb_folds

# Perform Hyperparameter tuning
doParallel::registerDoParallel()

set.seed(234)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)

xgb_res

# Explore metrics
collect_metrics(xgb_res)

# Plot metrics
xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  dplyr::select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

# Find and save best metrics
show_best(xgb_res, metric = "roc_auc")
best_auc <- select_best(xgb_res, metric = "roc_auc")
best_auc

# Finalize tuneable workflow
final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

final_xgb

# Find Parameters 
final_xgb %>%
    fit (data = vb_train) %>%
    pull_workflow_fit() %>%
    vip(geom = "point")

# Evaluate model using best fit
final_res <- last_fit(final_xgb, vb_split)
collect_metrics(final_res)

# Create ROC curve
final_res %>%
    collect_predictions() %>%
    roc_curve(truth = loan_paid_back,.pred_1, event_level = "second") %>%
    ggplot(aes(x = 1 - specificity, y = sensitivity)) +
    geom_line(size = 1.5, color = "midnightblue") +
    geom_abline(
      lty = 2, alpha = 0.5,
      color = "gray50",
      size = 1.2
    )

# Extract workflow
final_wf <- final_res %>% extract_workflow()

# Predict on test dataset
test_pred <- predict(final_wf, test_tree, type = "prob")
submission_xgb_iter02 <- data.frame(
  id = test_tree$id,
  loan_paid_back = test_pred$.pred_1
)

# Save to csv
write.csv(submission_xgb_iter02, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_iter02.csv", row.names = FALSE)
```


### XGBoost - Folds ensembled based Modeling using Target Encoding (Iteration 3)

This is my first attempt at advanced feature engineering as demonstrated by **Chris Deotte** in his [notebook](https://www.kaggle.com/code/cdeotte/feature-engineering-with-rapids-lb-38-847).

Earlier I tried to implement it separately and outside of my model training workflow. But I got limited success in that. So, I will be trying this again now within my training workflow using tidymodels.

Let's see how much success we get at this attempt. I have created following helper functions to perform advanced feature engineering:


- **Target encoding**: Encoding of categorical variables based on target variable

This code is written mostly by **ChatGPT** with bug fixes by me.

```{r xgb_cv_iter03}
# Load datasets
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_fe.Rdata")
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_fe.Rdata")

dt_train <- setDT(train_fe)
dt_test <- setDT(test_fe)

# Load target encoding function
source("/workspace/Projects/kaggle-credit-risk-s5e11/helpR_funcs/fe_target_encoder.R")

# Load ensemble fold cv model function
source("/workspace/Projects/kaggle-credit-risk-s5e11/helpR_funcs/ensemble_fold_cv_model.R")

# Define target variable 
target <- "loan_paid_back"

# Define id variable
id <- "id"

# Define non-features
non_features <- list(id, target)

# Categorise features
features <- names(dt_train[, .SD, .SDcols = setdiff(names(dt_train), non_features)])
numeric <- names(dt_train[, .SD, .SDcols = setdiff(names(dt_train[,.SD,.SDcol = is.numeric]), non_features)])
factors <- names(dt_train[,.SD, .SDcols = setdiff(names(dt_train), union(numeric, union(target, non_features)))])
inter <- colnames(dt_train %>% dplyr::select(starts_with("inter_")))
round <- colnames(dt_train %>% dplyr::select(starts_with("round_")))

# Specify XGBoost Params
xgb_params <- list(
  booster = "gbtree",
  
  # === Objective ===
  objective = "binary:logistic",
  eval_metric = "auc",
  
  # === Main hyperparameters ===
  eta = 0.01,               # learn_rate
  max_depth = 6,            # tree_depth
  subsample = 0.55,         # sample_size
  colsample_bytree = 0.3,   # mtry equivalent
  gamma = 0,                # loss_reduction
  
  # === GPU (CUDA) settings ===
  device = "cuda",
  
  # === System settings ===
  nthread = parallel::detectCores(),
  nrounds = 10000
)

# Perform Ensemble Fold CV Model Run 
res <- ensemble_fold_cv_model(
  train_dt = dt_train,
  test_dt  = dt_test,
  target   = target,

  INTER = inter,
  ROUND = round,
  CATS  = factors,

  xgb_params = xgb_params,
  folds = 5,
  te_cv = 5,
  te_smooth = 1.0,
  drop_original_te1 = TRUE,
  drop_original_te2 = FALSE,

  parallel = FALSE,
  seed = 2025,
  verbose = TRUE
)

# Extract submissions
submission_xgb_iter03 <- data.frame(
  id = test_tree$id,
  loan_paid_back = test_pred$.pred_1
)

# Save to csv
write.csv(submission_xgb_iter03, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_iter02.csv", row.names = FALSE)
```

```{r validation_te}
N <- nrow(dt_train)
split_idx <- floor(0.8 * N)

dt_train_v <- dt_train[1:split_idx]
dt_valid_v <- dt_train[(split_idx+1):N]

y_train_v <- dt_train_v[[target]]
y_valid_v <- dt_valid_v[[target]]

dt_train_te <- te_fit_transform(
  dt_train_v,
  cols = inter,
  target = target,
  aggs = "mean",
  smooth = 1,
  debug = TRUE,
  drop_original = TRUE
)

te_obj <- te_fit(
  dt_train_v,
  cols = inter,
  target = target,
  aggs = "mean",
  smooth = 1,
  debug = TRUE
)

dt_valid_te <- te_transform(
  te_obj,
  dt_valid_v,
  drop_original = TRUE,
  in_place = FALSE,
  debug = TRUE
)

# Compare


x_te_k <- fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/X_train_te.csv")
v_te_k <- fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/X_valid_te.csv")

x_te_r <- dt_train_te[[1]]

#INTER
te_inter <- colnames(x_te_r %>% dplyr::select(starts_with("TE_inter_")))
DT1 <- x_te_k[,.SD, .SDcols = te_inter]
DT2 <- x_te_r[,.SD, .SDcols = te_inter]

compare_cols <- data.table::data.table(
  column = names(DT1),
  class_DT1 = sapply(DT1, class),
  class_DT2 = sapply(DT2, class),
  same_class = sapply(DT1, class) == sapply(DT2, class),
  identical_col = sapply(names(DT1), function(col) all.equal(DT1[[col]], DT2[[col]])),
  unique_DT1 = sapply(DT1, function(x) length(unique(x))),
  unique_DT2 = sapply(DT2, function(x) length(unique(x)))
)

compare_cols

# Find mismatch
find_mismatches <- function(DT1, DT2, col) {
  idx <- DT1[[col]] != DT2[[col]] | (is.na(DT1[[col]]) != is.na(DT2[[col]]))
  mism <- data.table::data.table(
    row = which(idx),
    DT1_value = DT1[[col]][idx],
    DT2_value = DT2[[col]][idx]
  )
  return(mism)
}

mism <- find_mismatches(DT1, DT2, "TE_inter_interest_rate_gender_mean")
head(mism, 20)
nrow(mism)
```

```{r te_check1}
fold_ids_r <- kfold_ids(nrow(dt_train), v=5, seed=42)
fold_ids_py <- fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/py_folds.csv")
mean(py$V1 == r$V1)    # % match

```

```{r kfold}
kfold_ids_sklearn <- function(n, v = 5, seed = 42) {
  set.seed(seed)
  
  # Step 1 — shuffle like sklearn
  idx <- sample.int(n) - 1   # create 0-based shuffled indices
  
  # Step 2 — calculate fold sizes like sklearn
  fold_sizes <- rep(floor(n / v), v)
  remainder <- n %% v
  fold_sizes[1:remainder] <- fold_sizes[1:remainder] + 1
  
  # Step 3 — assign folds
  fold_id <- integer(n)
  start <- 1
  
  for (i in seq_len(v)) {
    end <- start + fold_sizes[i] - 1
    fold_id[idx[start:end] + 1] <- i - 1  # assign fold # (0..v-1)
    start <- end + 1
  }
  
  return(fold_id)
}

fold_r <- kfold_ids_sklearn(nrow(dt_train), v=5, seed=42)
```