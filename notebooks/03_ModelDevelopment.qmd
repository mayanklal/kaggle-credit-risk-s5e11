---
title: "Credit Risk Scorecard Model - Model Development"
author: "Mayank Lal"
date: today
format: 
    html:
        code-fold: true
        code-summary: "Code"
        code-overflow: wrap
execute:
  echo: false
  include: false
  warning: false
---
```{r setup}
## Exploratory Data Analysis
library(tidyverse) # Data Manipulation and Visualizaton
library(data.table) # Large datasets
library(vtable) # Formatted Summary Table
library(corrplot) # Correlation
library(patchwork) # Grid of plots
library(scales) # For Percentage Labels
library(rpart) # Simple Decision Tree for interaction analysis
library(rpart.plot) # Plotting Decision Trees

## Variable Selection and Feature Engineering
library(scorecard) # Information Value
library(pROC) # Gini
library(gt) # Formattting Table for publishing
library(knitr) # For simple formatting
library(DT) # For interactive tables
library(stringr) # Selecting sub-strings
library(Hmisc) # Variabel Clustering
library(car) # VIF
library(glmnet) # LASSO

# Model Development
library(MASS) # Stepwise Regression
library(lmtest) # Joint Wald Test
library(Matrix) # Create Matrix objects for XGBoost
library(xgboost) # XGBoost
library(caret) # Stratified Splitting
library(SHAPforxgboost) # Interpretability

# Tidy Modeling
library(tidymodels)
library(recipes)
library(moments)
library(rlang)
library(vip)
```

This is a rough sketch notebook. We will furnish it before publishing.
```{r setup}
set.seed(123)
```

```{r data}
# Load Model Data for Logistic Regression
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_final_lr.Rdata")

# Load Test Data for Logistic Regression
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_final_lr.Rdata")

# Load Model Data for Tree-based Models
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_tree.Rdata")
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_tree_encoded.Rdata")
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_tree_agg.Rdata")

# Load Test Data for Tree-based Models
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_tree.Rdata")
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_tree_encoded.Rdata")
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_tree_agg.Rdata")
```

## Model Development

Let's start with model development. First, we will use stepwise logistic regression to select final list of variables that improves AIC.

### Logistic Regression - Stepwise Regression (Iteration 0)

```{r lr_step}
# Create list of variables
vars <- c("working_flag_woe",
          "debt_to_income_ratio_woe",
          "credit_score_woe",
          "interest_rate_woe",
          "education_level_woe",
          "loan_purpose_woe",
          "annual_income_woe")

# Full model formula
full_formula <- as.formula(
  paste("loan_paid_back ~", paste(vars, collapse = " + "))
)

# Null model (only intercept)
null_formula <- loan_paid_back ~ 1

stepwise_model <- stepAIC(
    glm(loan_paid_back ~ ., data = train_final_lr, family = binomial),
    scope=list(lower=null_formula, upper=full_formula),
    direction="both",
    trace=TRUE
)

summary(stepwise_model)
```

### Logistic Regression - Final Model (Iteration 1)

```{r lr_final}
# Create list of final variables after stepwise regression
final_model_vars <- names(coef(stepwise_model))[-1]

# Final model formula
final_formula <- as.formula(
  paste("loan_paid_back ~", paste(final_model_vars, collapse = " + "))
)

# Build Final Logistic Regression Model
final_model <- glm(final_formula, data = train_final_lr, family = binomial)
summary(final_model)
```


#### Perform wald test for individual predictors

```{r wald_indv}
Anova(final_model)
```

### Logistic Regression - Final Model with interactions (Iteration 2)
```{r lr_interaction}
# Create interaction terms
interaction_terms <- c("debt_to_income_ratio_woe:credit_score_woe", "credit_score_woe:interest_rate_woe")

# Create final formula with interactions
final_formula_interaction <- as.formula(
  paste("loan_paid_back ~",
        paste(final_model_vars, collapse = " + "),
        "+",
        paste(interaction_terms, collapse = " + "))
)

# Build final model with interactions
final_model_interaction <- glm(final_formula_interaction,
                               data=train_final_lr,
                               family=binomial)
summary(final_model_interaction)
```

#### Perform wald joint test

```{r wald_joint}
waldtest(final_model, final_model_interaction)
```

### XGBoost - Base Model using k-fold cross-validation (Iteration 1)

In base model, we will be adding all variables in their raw form without any feature engineering.

Let's see the AUC of this base model. 

```{r xgb_cv_iter01}
# Create train and validation split 
train_idx <- createDataPartition(train_tree$loan_paid_back, p = 0.8, list = FALSE)
train_xgb <- train_tree[train_idx]
valid_xgb <- train_tree[-train_idx]

# Full formula 
feature_formula <- ~ . - id - loan_paid_back - 1

# Convert to DMatrix 
train_xgb_matrix <- model.matrix(feature_formula, data = train_xgb)
valid_xgb_matrix <- model.matrix(feature_formula, data = valid_xgb)

dtrain <- xgb.DMatrix(train_xgb_matrix, label = train_xgb$loan_paid_back)
dvalid <- xgb.DMatrix(valid_xgb_matrix, label = valid_xgb$loan_paid_back)

# Estimate Weight 
neg <- sum(train_xgb$loan_paid_back == 0)
pos <- sum(train_xgb$loan_paid_back == 1)
scale_pos_weight <- neg/pos

# Baseline parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.03,
  max_depth = 5,
  min_child_weight = 30,
  subsample = 0.8,
  colsample_bytree = 0.6,
  gamma = 1,
  lambda = 2,
  scale_pos_weight = scale_pos_weight
)

# Perform k-fold Cross-validation
xgb_cv <- xgb.cv(
  data = dtrain,
  param = params,
  nrounds = 5000,
  nfold = 10,
  early_stopping_rounds = 50,
  maximize = TRUE,
  verbose = 1
)

best_nrounds <- xgb_cv$early_stop$best_iteration

# Train Base Model
xgb_base_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  evals = list(train=dtrain, valid=dvalid),
  early_stopping_rounds = 50
)

# Predict on test dataset
feature_formula_test <- ~ . - id - 1
test_xgb_matrix <- model.matrix(feature_formula_test, data = test_tree)
test_pred_xgb  <- predict(xgb_base_model, test_xgb_matrix)
submission_xgb_itr01 <- data.frame(
  id = test_tree$id,
  loan_paid_back = test_pred_xgb
)

# Save to csv
write.csv(submission_xgb_itr01, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_iter01.csv", row.names = FALSE)

# Feature Importance 
importance_matrix <- xgb.importance(
  model = xgb_base_model,
  feature_names = colnames(dtrain)
)

xgb.plot.importance(importance_matrix)
```

### XGBoost - Base Model with encoding of categorical variables (Iteration 2)

```{r xgb_cv_iter02}
# Create train and validation split 
train_idx <- createDataPartition(train_tree_encoded$loan_paid_back, p = 0.8, list = FALSE)
train_xgb <- train_tree_encoded[train_idx]
valid_xgb <- train_tree_encoded[-train_idx]

# Full formula 
feature_formula <- ~ . - id - loan_paid_back - 1

# Convert to DMatrix 
train_xgb_matrix <- model.matrix(feature_formula, data = train_xgb)
valid_xgb_matrix <- model.matrix(feature_formula, data = valid_xgb)

dtrain <- xgb.DMatrix(train_xgb_matrix, label = train_xgb$loan_paid_back)
dvalid <- xgb.DMatrix(valid_xgb_matrix, label = valid_xgb$loan_paid_back)

# Estimate Weight 
neg <- sum(train_xgb$loan_paid_back == 0)
pos <- sum(train_xgb$loan_paid_back == 1)
scale_pos_weight <- neg/pos

# Baseline parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.03,
  max_depth = 5,
  min_child_weight = 30,
  subsample = 0.8,
  colsample_bytree = 0.6,
  gamma = 1,
  lambda = 2,
  scale_pos_weight = scale_pos_weight
)

# Perform k-fold Cross-validation
xgb_cv <- xgb.cv(
  data = dtrain,
  param = params,
  nrounds = 5000,
  nfold = 10,
  early_stopping_rounds = 50,
  maximize = TRUE,
  verbose = 1
)

best_nrounds <- xgb_cv$early_stop$best_iteration

# Train Model
xgb_model_post_encode <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  evals = list(train=dtrain, valid=dvalid),
  early_stopping_rounds = 50
)

# Predict on test dataset
feature_formula_test <- ~ . - id - 1
test_xgb_matrix <- model.matrix(feature_formula_test, data = test_tree_encoded)
test_pred_xgb  <- predict(xgb_model_post_encode, test_xgb_matrix)
submission_xgb_itr02 <- data.frame(
  id = test_tree_encoded$id,
  loan_paid_back = test_pred_xgb
)

# Save to csv
write.csv(submission_xgb_itr02, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_iter02.csv", row.names = FALSE)

# Feature Importance 
importance_matrix <- xgb.importance(
  model = xgb_model_post_encode,
  feature_names = colnames(dtrain)
)

xgb.plot.importance(importance_matrix)
```

### XGBoost - Aggregating Statistics of Numerical Variable by Categorical Variables (Iteration 3)

```{r xgb_cv_iter03}
# Create train and validation split 
train_idx <- createDataPartition(train_tree_agg$loan_paid_back, p = 0.8, list = FALSE)
train_xgb <- train_tree_agg[train_idx]
valid_xgb <- train_tree_agg[-train_idx]

# Full formula 
feature_formula <- ~ . - id - loan_paid_back - 1

# Convert to DMatrix 
train_xgb_matrix <- model.matrix(feature_formula, data = train_xgb)
valid_xgb_matrix <- model.matrix(feature_formula, data = valid_xgb)

dtrain <- xgb.DMatrix(train_xgb_matrix, label = train_xgb$loan_paid_back)
dvalid <- xgb.DMatrix(valid_xgb_matrix, label = valid_xgb$loan_paid_back)

# Estimate Weight 
neg <- sum(train_xgb$loan_paid_back == 0)
pos <- sum(train_xgb$loan_paid_back == 1)
scale_pos_weight <- neg/pos

# Baseline parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  #tree_method = "gpu_hist",
  #predictor = "gpu_predictor",
  max_depth = 5,
  eta = 0.03,
  min_child_weight = 50, # Incresed from 30 to 50 to reduce overfitting
  subsample = 0.8,
  colsample_bytree = 0.8, # Increased from 0.6 to 0.8 
  gamma = 1,
  lambda = 2,
  scale_pos_weight = scale_pos_weight
)

# Perform k-fold Cross-validation
xgb_cv <- xgb.cv(
  data = dtrain,
  param = params,
  nrounds = 5000,
  nfold = 10,
  early_stopping_rounds = 50,
  maximize = TRUE,
  verbose = 1
)

best_nrounds <- xgb_cv$early_stop$best_iteration

# Train Model
xgb_model_agg <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  evals = list(train=dtrain, valid=dvalid),
  early_stopping_rounds = 50
)

# Predict on test dataset
feature_formula_test <- ~ . - id - 1
test_xgb_matrix <- model.matrix(feature_formula_test, data = test_tree_agg)
test_pred_xgb  <- predict(xgb_model_agg, test_xgb_matrix)
submission_xgb_itr03 <- data.frame(
  id = test_tree_agg$id,
  loan_paid_back = test_pred_xgb
)

# Save to csv
write.csv(submission_xgb_itr03, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_iter03.csv", row.names = FALSE)

# Feature Importance 
importance_matrix <- xgb.importance(
  model = xgb_model_agg,
  feature_names = colnames(dtrain)
)

xgb.plot.importance(importance_matrix[1:20,])
```

### XGBoost - Tidymodels (Iteration 4)

Based on discussions on Kaggle, I realized that my model development workflow is heavily flawed. <Will elaborate on why is it flawed later on?> To bridge the gap between my workflow and my peers' workflow, I will now use tidymodels based workflow. <Will elaborte on what is tidymodels and how it helps later on>

```{r xgb_cv_iter04}
# Prepare datasets
vb_df <- train_tree_encoded %>% dplyr::select(-id)
vb_df$loan_paid_back <- factor(vb_df$loan_paid_back, levels = c(0, 1))
vb_split <- initial_split(vb_df, strata = loan_paid_back)
vb_train <- training(vb_split)
vb_test <- testing(vb_split)

# Specify paramters of XGBOost Model that needs to be tuned
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(), min_n = tune(),
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost", eval_metric = "auc") %>%
  set_mode("classification")

xgb_spec

# Specify hyperparameter space to find values for hyperparameters
xgb_grid <- grid_space_filling(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), vb_train),
  learn_rate(),
  size = 30
)

xgb_grid

# Specify Workflow
xgb_wf <- workflow() %>%
  add_formula(loan_paid_back ~ .) %>%
  add_model(xgb_spec)

xgb_wf

# Specify folds for k-fold cross-validation
set.seed(123)
vb_folds <- vfold_cv(vb_train, strata = loan_paid_back, v = 7)

vb_folds

# Perform Hyperparameter tuning
doParallel::registerDoParallel()

set.seed(234)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)

xgb_res

# Explore metrics
collect_metrics(xgb_res)

# Plot metrics
xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

# Find and save best metrics
show_best(xgb_res, "roc_auc")
best_auc <- select_best(xgb_res, "roc_auc")
best_auc

# Finalize tuneable workflow
final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

final_xgb
```