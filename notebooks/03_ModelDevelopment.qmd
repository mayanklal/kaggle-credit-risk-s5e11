---
title: "Credit Risk Scorecard Model - Model Development"
author: "Mayank Lal"
date: today
format: 
    html:
        code-fold: true
        code-summary: "Code"
        code-overflow: wrap
execute:
  echo: false
  include: false
  warning: false
---
```{r setup}
## Exploratory Data Analysis
library(tidyverse) # Data Manipulation and Visualizaton
library(data.table) # Large datasets
library(vtable) # Formatted Summary Table
library(corrplot) # Correlation
library(patchwork) # Grid of plots
library(scales) # For Percentage Labels
library(rpart) # Simple Decision Tree for interaction analysis
library(rpart.plot) # Plotting Decision Trees

## Variable Selection and Feature Engineering
library(scorecard) # Information Value
library(pROC) # Gini
library(gt) # Formattting Table for publishing
library(knitr) # For simple formatting
library(DT) # For interactive tables
library(stringr) # Selecting sub-strings
library(Hmisc) # Variabel Clustering
library(car) # VIF
library(glmnet) # LASSO

# Model Development
library(MASS) # Stepwise Regression
library(lmtest) # Joint Wald Test
library(Matrix) # Create Matrix objects for XGBoost
library(xgboost) # XGBoost
library(caret) # Stratified Splitting
library(SHAPforxgboost) # Interpretability
```

This is a rough sketch notebook. We will furnish it before publishing.
```{r setup}
set.seed(123)
```

```{r data}
# Load Model Data for Logistic Regression
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_final_lr.Rdata")

# Load Test Data for Logistic Regression
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_final_lr.Rdata")

# Load Model Data for Tree-based Models
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_tree_final.Rdata")
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_tree_final_te.Rdata")

# Load Test Data for Tree-based Models
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_tree_final.Rdata")
load(file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_tree_final_te.Rdata")
```

## Model Development

Let's start with model development. First, we will use stepwise logistic regression to select final list of variables that improves AIC.

### Logistic Regression - Stepwise Regression (Iteration 0)

```{r lr_step}
# Create list of variables
vars <- c("working_flag_woe",
          "debt_to_income_ratio_woe",
          "credit_score_woe",
          "interest_rate_woe",
          "education_level_woe",
          "loan_purpose_woe",
          "annual_income_woe")

# Full model formula
full_formula <- as.formula(
  paste("loan_paid_back ~", paste(vars, collapse = " + "))
)

# Null model (only intercept)
null_formula <- loan_paid_back ~ 1

stepwise_model <- stepAIC(
    glm(loan_paid_back ~ ., data = train_final_lr, family = binomial),
    scope=list(lower=null_formula, upper=full_formula),
    direction="both",
    trace=TRUE
)

summary(stepwise_model)
```

### Logistic Regression - Final Model (Iteration 1)

```{r lr_final}
# Create list of final variables after stepwise regression
final_model_vars <- names(coef(stepwise_model))[-1]

# Final model formula
final_formula <- as.formula(
  paste("loan_paid_back ~", paste(final_model_vars, collapse = " + "))
)

# Build Final Logistic Regression Model
final_model <- glm(final_formula, data = train_final_lr, family = binomial)
summary(final_model)
```


#### Perform wald test for individual predictors

```{r wald_indv}
Anova(final_model)
```

### Logistic Regression - Final Model with interactions (Iteration 2)
```{r lr_interaction}
# Create interaction terms
interaction_terms <- c("debt_to_income_ratio_woe:credit_score_woe", "credit_score_woe:interest_rate_woe")

# Create final formula with interactions
final_formula_interaction <- as.formula(
  paste("loan_paid_back ~",
        paste(final_model_vars, collapse = " + "),
        "+",
        paste(interaction_terms, collapse = " + "))
)

# Build final model with interactions
final_model_interaction <- glm(final_formula_interaction,
                               data=train_final_lr,
                               family=binomial)
summary(final_model_interaction)
```

#### Perform wald joint test

```{r wald_joint}
waldtest(final_model, final_model_interaction)
```

### XGBoost - Base Model using k-fold cross-validation (Iteration 1)

In base model, we will be adding numeric variables in their raw form, factor variables in their one-hot encoded form and transformed form with key interaction terms that we utilized in our logistic regression model as well. These interaction terms are DTI x Credit Score and Credit Score x Interest Rate.

Let's see the AUC of this base model. 

```{r xgb_cv_iter1}
# Create train and validation split 
train_idx <- createDataPartition(train_tree_final$loan_paid_back, p = 0.8, list = FALSE)
train_xgb <- train_tree_final[train_idx]
valid_xgb <- train_tree_final[-train_idx]

# Full formula 
feature_formula <- ~ annual_income + debt_to_income_ratio + credit_score + interest_rate + education_level + loan_purpose + employment_status + grade_subgrade + working_flag + grade + loan_purpose_main + education_level_main + grade_factor + dti_cs + ir_cs - 1

# Convert to DMatrix 
train_xgb_matrix <- model.matrix(feature_formula, data = train_xgb)
valid_xgb_matrix <- model.matrix(feature_formula, data = valid_xgb)

dtrain <- xgb.DMatrix(train_xgb_matrix, label = train_xgb$loan_paid_back)
dvalid <- xgb.DMatrix(valid_xgb_matrix, label = valid_xgb$loan_paid_back)

# Estimate Weight 
neg <- sum(train_xgb$loan_paid_back == 0)
pos <- sum(train_xgb$loan_paid_back == 1)
scale_pos_weight <- neg/pos

# Baseline parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.03,
  max_depth = 5,
  min_child_weight = 30,
  subsample = 0.8,
  colsample_bytree = 0.6,
  gamma = 1,
  lambda = 2,
  scale_pos_weight = scale_pos_weight
)

# Perform k-fold Cross-validation
xgb_cv <- xgb.cv(
  data = dtrain,
  param = params,
  nrounds = 5000,
  nfold = 10,
  early_stopping_rounds = 50,
  maximize = TRUE,
  verbose = 1
)

best_nrounds <- xgb_cv$early_stop$best_iteration

# Train Base Model
xgb_base_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  evals = list(train=dtrain, valid=dvalid),
  early_stopping_rounds = 50
)

# Predict on test dataset
test_xgb_matrix <- model.matrix(feature_formula, data = test_tree_final)
test_pred_xgb  <- predict(xgb_base_model, test_xgb_matrix)
submission_xgb_itr01 <- data.frame(
  id = test_tree_final$id,
  loan_paid_back = test_pred_xgb
)

# Save to csv
write.csv(submission_xgb_itr01, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_base.csv", row.names = FALSE)
```

### XGBoost - Base Model with full set of features using k-fold cross-validation (Iteration 2)

```{r xgb_cv_iter2}
# Create train and validation split 
train_idx <- createDataPartition(train_tree_final$loan_paid_back, p = 0.8, list = FALSE)
train_xgb <- train_tree_final[train_idx]
valid_xgb <- train_tree_final[-train_idx]

# Full formula 
feature_formula <- ~ . - id - loan_paid_back - 1

# Convert to DMatrix 
train_xgb_matrix <- model.matrix(feature_formula, data = train_xgb)
valid_xgb_matrix <- model.matrix(feature_formula, data = valid_xgb)

dtrain <- xgb.DMatrix(train_xgb_matrix, label = train_xgb$loan_paid_back)
dvalid <- xgb.DMatrix(valid_xgb_matrix, label = valid_xgb$loan_paid_back)

# Estimate Weight 
neg <- sum(train_xgb$loan_paid_back == 0)
pos <- sum(train_xgb$loan_paid_back == 1)
scale_pos_weight <- neg/pos

# Baseline parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.03,
  max_depth = 5,
  min_child_weight = 30,
  subsample = 0.8,
  colsample_bytree = 0.6,
  gamma = 1,
  lambda = 2,
  scale_pos_weight = scale_pos_weight
)

# Perform k-fold Cross-validation
xgb_cv <- xgb.cv(
  data = dtrain,
  param = params,
  nrounds = 5000,
  nfold = 10,
  early_stopping_rounds = 50,
  maximize = TRUE,
  verbose = 1
)

best_nrounds <- xgb_cv$early_stop$best_iteration

# Train Base Model with interactions
xgb_base_model_with_interactions <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  evals = list(train=dtrain, valid=dvalid),
  early_stopping_rounds = 50
)

# Predict on test dataset
feature_formula_test <-  ~ . - id - 1
test_xgb_matrix <- model.matrix(feature_formula_test, data = test_tree_final)
test_pred_xgb  <- predict(xgb_base_model_with_interactions, test_xgb_matrix)
submission_xgb_iter02 <- data.frame(
  id = test_tree_final$id,
  loan_paid_back = test_pred_xgb
)

# Save to csv
write.csv(submission_xgb_iter02, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_full.csv", row.names = FALSE)
```


### XGBoost - Base Model with full set of features and target encoding using k-fold cross-validation (Iteration 3)

```{r xgb_cv_iter3}
# Create train and validation split 
train_idx <- createDataPartition(train_tree_final_te$loan_paid_back, p = 0.8, list = FALSE)
train_xgb <- train_tree_final_te[train_idx]
valid_xgb <- train_tree_final_te[-train_idx]

# Full formula 
feature_formula <- ~ . - id - loan_paid_back - 1

# Convert to DMatrix 
train_xgb_matrix <- model.matrix(feature_formula, data = train_xgb)
valid_xgb_matrix <- model.matrix(feature_formula, data = valid_xgb)

dtrain <- xgb.DMatrix(train_xgb_matrix, label = train_xgb$loan_paid_back)
dvalid <- xgb.DMatrix(valid_xgb_matrix, label = valid_xgb$loan_paid_back)

# Estimate Weight 
neg <- sum(train_xgb$loan_paid_back == 0)
pos <- sum(train_xgb$loan_paid_back == 1)
scale_pos_weight <- neg/pos

# Baseline parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.03,
  max_depth = 5,
  min_child_weight = 30,
  subsample = 0.8,
  colsample_bytree = 0.6,
  gamma = 1,
  lambda = 2,
  scale_pos_weight = scale_pos_weight
)

# Perform k-fold Cross-validation
xgb_cv <- xgb.cv(
  data = dtrain,
  param = params,
  nrounds = 5000,
  nfold = 10,
  early_stopping_rounds = 50,
  maximize = TRUE,
  verbose = 1
)

best_nrounds <- xgb_cv$early_stop$best_iteration

# Train Base Model with Advanced Feature Engineering
xgb_base_model_afe <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = best_nrounds,
  evals = list(train=dtrain, valid=dvalid),
  early_stopping_rounds = 50
)

# Predict on test dataset
feature_formula_test <-  ~ . - id - 1
test_xgb <- model.matrix(feature_formula_test, data = test_tree_final_te)
test_pred_xgb  <- predict(xgb_base_model_afe, test_xgb)
submission_xgb_iter03 <- data.frame(
  id = test_tree_final_te$id,
  loan_paid_back = test_pred_xgb
)

# Save to csv
write.csv(submission_xgb_iter03, "/workspace/Projects/kaggle-credit-risk-s5e11/outputs/submission_xgb_full_te.csv", row.names = FALSE)
```

