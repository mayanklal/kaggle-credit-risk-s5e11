---
title: "Credit Risk Scorecard Model - Feature Engineering"
author: "Mayank Lal"
date: today
format: 
    html:
        code-fold: true
        code-summary: "Code"
        code-overflow: wrap
execute:
  echo: false
  include: false
  warning: false
---
```{r setup}
## Exploratory Data Analysis
library(tidyverse) # Data Manipulation and Visualizaton
library(data.table) # Large datasets
library(vtable) # Formatted Summary Table
library(corrplot) # Correlation
library(patchwork) # Grid of plots
library(scales) # For Percentage Labels
library(rpart) # Simple Decision Tree for interaction analysis
library(rpart.plot) # Plotting Decision Trees

## Variable Selection and Feature Engineering
library(scorecard) # Information Value
library(pROC) # Gini
library(gt) # Formattting Table for publishing
library(knitr) # For simple formatting
library(DT) # For interactive tables
library(stringr) # Selecting sub-strings
library(Hmisc) # Variabel Clustering
library(car) # VIF
library(glmnet) # LASSO
library(janitor) # Cleaning names
library(caret) # Create Folds
library(future)
```

```{r setup}
set.seed(123)
```
```{r data}
# Load Data
train <- fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/raw/train.csv")
test <- fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/raw/test.csv")
orig <-fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/raw/loan_dataset_20000.csv")
```


```{r common_funcs}
plot_bubble_chart <- function(data, var, target_var, title, x_label) {
bubble <- data %>%
  group_by({{var}}) %>%
  summarise(
    good_rate = mean({{target_var}} == 1),
    count = n()
  ) %>%
  ggplot(aes(x = {{var}}, y = good_rate, size = count)) +
  geom_point(alpha = 0.5) +
  scale_size(range = c(10, 20), name = "No. of Customers") +
  labs(
        title = title,
        x = x_label,
        y = "Loan Repayment Rate"
    ) +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10))
return(bubble)
}
```

## Variable Selection

We begin this journey by selecting variables for model development using regression and tree-based techniques. 

First, we will look at Information Value.

### Information Value

In this section, we will calculate Information Value to filter variables for regression / scorecard modeling. <To be elaborated further>

```{r information_value, include = TRUE}
vars <- colnames(train)[-1]
iv_table <- iv(train[, ..vars], y = "loan_paid_back")
kable(iv_table, caption = "Information Value of Variables")
```

If this wasn't a kaggle competition with a synthetic dataset, we would have only selected variables with IV > 0.02. This would have given us following variables:

- Employment Status
- DTI
- Loan Amount
- Annual Income
- Credit Score
- Grade Sub-grade
- Interest Rate

But since it is a kaggle competition, we would not be enforcing such a hard criterion for filtering variables. We will combine IV with our business intuition to select following variables:

- Employment Status
- DTI
- Loan Amount
- Annual Income
- Credit Score
- Grade Sub-grade
- Interest Rate
- **Loan Purpose**
- **Education Level**

Why have we added **Loan Purpose** and **Education Level**? We have added them back based on our EDA analysis in which we saw a clear and noticeable difference in loan repayment rate between different classes of these variables. Also, they make business intuitive sense as well. 

**Loan Purpose** will help us understand if a loan taken for a given purpose has higher likelihood of repayment or not.  **Education Level** can be an indicator of future employability and income of the customer reducing the likelihood of loan default. Also, there might be an interaction between annual income and education level. We will investigate this in later phase.

Also, one thing to note here is that IV of Employment Status and DTI is very suspiciously high (>0.5). An IV this high is rare in real-world credit datasets. It signals:

- Possibility of data leakage
- Variable may be directly encoding the target
- Or near-direct proxy for “good/bad”

But since this is a synthetic dataset on Kaggle, it can be expected. For sanctity sake, we will also perform data leakage check later on.

### Clustering-based Filtering

In this section, we will perform process to filter variables using **Hierarchical Clustering**.

Before performing clustering, we will have to transform categorical variables to numerical variables. After IV-based filtering, there are four categorical variables: **Employment Status**, **Grade Sub-grade**, **Education Level** and **Loan Purpose**

First let's transform **Employment Status**. As we have seen in our EDA, the loan repayment rate is very high for three classes - **Retired, Employed and Self-employed**. Similarly, it is ver low for remaining two classes - **Student and Unemployed**. Given the clear separation, we can merge them into two groups and then apply **WOE encoding** to ensure that we don't lose any information.

```{r employment_status, include = TRUE}
# Create Working Flag
train$working_flag <- ifelse(train$employment_status %in% c("Retired", "Employed", "Self-employed"),1 ,0)
test$working_flag <- ifelse(test$employment_status %in% c("Retired", "Employed", "Self-employed"),1 ,0)

# Apply WOE Encoding
es_bins <- woebin(
  train,
  y = "loan_paid_back",
  x = "working_flag"
)

# Plot WOE Bins of Employment Status/Working Flag
woebin_plot(es_bins)
```

Now, we will transform **Grade Sub-grade** variable. As we have seen in our EDA analysis, they have monotonic relationship with loan repayment rate. Also, it is an ordinal variable with clear order: **A>B>C>D>E>F**. Therefore, we will try to merge the sub-grades into one grade and then convert into an ordinal factor variable. Then finally into a numeric variable.

After that during feature engineering phase, we will try to find which transformation or encoding will be suitable for it - **WOE Encoding** or **Target Encoding**.

```{r grade, include = TRUE}
# Merge Sub-grades to Grades using ordinal factor conversion
train$grade <- str_sub(train$grade_subgrade, 1, 1)
train$grade <- factor(
  train$grade,
  levels = c("A", "B", "C", "D", "E", "F"),
  labels = c(6,5,4,3,2,1),
  ordered = FALSE
)

train$grade <- as.numeric(train$grade)


test$grade <- str_sub(test$grade_subgrade, 1, 1)
test$grade <- factor(
  test$grade,
  levels = c("A", "B", "C", "D", "E", "F"),
  ordered = TRUE
)

test$grade <- as.numeric(test$grade)

# Plot Bubble plot of Grade
#| fig.width: 12
#| fig.height: 8
#| fig.align: "center"
plot_bubble_chart(train, grade, loan_paid_back, "Bubble Plot of Grade by Loan Repayment Rate", "Grade")
```
As we can see from above chart, the Sub-grades have been merged to their Grade and converted to numeric.

Next we will transform both **Education Level** and **Loan Purpose**. Both of these variables have multiple classes and don't have definite order. There is slight variation in loan repayment rate of each class of these variables. Like previously we have done with **Employment Status**, we would be applying **WOE encoding** to convert them to numeric while preserving the signal embedded in them.

```{r education_loan_purpose, include = TRUE}
# Apply WOE Encoding
ed_lp_bins <- woebin(
  train,
  y = "loan_paid_back",
  x = c("education_level", "loan_purpose")
)

# Plot WOE Bins of Education Level and Loan Purpose
woebin_plot(ed_lp_bins)
```

From the above charts, we can see that we are able to maintain monotonic relationship through **WOE Encoding**. It automatically merges classes within a variable to attain that. This will be used while performing modeling using logistic regression as well. Now, we will perform **Hierarchical Clustering**.

```{r hc, include = TRUE}
# Filter variables based on IV statistic and Business Intuition
train_corr <- train %>% select(-employment_status, 
                               -grade_subgrade,
                               -gender,
                               -marital_status)

test_corr <- test %>% select(-employment_status, 
                               -grade_subgrade,
                               -gender,
                               -marital_status)

# Perform WoE Binning to Education Level, Loan Purpose and Working Flag
bins <- woebin(
  train_corr,
  y = "loan_paid_back",
  x = c("education_level", "loan_purpose", "working_flag")
)

# Apply WoE Encoding to Education Level, Loan Purpose and Working Flag
train_corr <- woebin_ply(train_corr, bins)
test_corr <- woebin_ply(test_corr, bins)


# Create correlation matrix
corr_matrix <- abs(cor(train_corr[, c(-1, -7)], use = "pairwise.complete.obs"))

# Create distance matrix
dist_matrix <- as.dist(1 - corr_matrix)

# Perform hierarchical clustering
hc <- hclust(dist_matrix, method = "complete")

# Plot Dendogram 
plot(hc, main = "Variable Clustering Dendogram")
```

From the above dendogram plot, we can see that there are **two key clutsters** as shown below:

- **Credit Score**, **Grade**, and **Interest Rate**
- **Debt to Income Ratio** and **Working Flag**

From each of these clusters, we can select one variable with highest IV value. But before doing that we can also check their correlation and our business intuition. Based on both of them, we will decide which variables need to be kept for further processing and feature engineering.

We will recalculate Information Value (IV) to determine if we need to make a decision about filtering out any variables from each of the above clusters.

```{r hc_vars, include=TRUE}
# Cut tree at correlation threshold of 0.75
clusters <- cutree(hc, h = 0.75)

# Recompute IV
iv_table_transformed <- iv(train_corr[,-1], y = "loan_paid_back")

# Build cluster table with IV 
cluster_df <- data.frame(variable = colnames(train_corr[,c(-1,-7)]),
                         cluster = clusters) %>%
                         left_join(iv_table_transformed, by = "variable") %>%
                         arrange(cluster, desc(info_value))
kable(cluster_df)
```

Based on above table, we will keep all the variables for now. Though we should keep in mind that we can eliminate two variables from Cluster 3 if required later on.

### Multicollinearity

In this section, we will perform filtering using **Variance Inflation Factor (VIF)** statistic. VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity between predictors.

If a variable is strongly correlated with other predictors, its coefficient becomes unstable and its standard error becomes larger.
VIF quantifies this effect. <Add Mathematical Formula as well>

```{r vif, include = TRUE}
# Define target, id, and feature variables
target_var <- "loan_paid_back"
id_var <- "id"
features_train <- train_corr[, !(names(train_corr) %in% c(id_var))]

# Keep only features
train_features <- train_corr[,..features_train]

# Estimate VIF using simple linear regression model
lm_model <- lm(loan_paid_back ~ ., data = train_features)

# Calculate VIF
vif_values <- vif(lm_model)
vif_df <- data.frame(
  variable = names(vif_values),
  vif = as.numeric(vif_values),
  row.names = NULL
) %>% arrange(desc(vif))
kable(vif_df)
```

Based on above table, we can finally make the decision to drop **Grade** due to multicollinearity (VIF > 5) with **Credit Score** and **Interest Rate**. Also, Grade can be considered as discretized version of credit score which adds no information to our model. Therefore, we will be dropping **Grade** from our model.

Let's recalculate VIF after dropping **Grade**.

```{r vif2, include = TRUE}
# Drop Grade
features_train_v2 <- train_features[, !(names(train_features) %in% c("grade"))]
features_test_v2 <- test_corr[, !(names(test_corr) %in% c("grade"))]

# Keep only features
train_features_v2 <- train_features[,..features_train_v2]
test_features_v2 <- test_corr[,..features_test_v2]

# Estimate VIF using simple linear regression model
lm_model_v2 <- lm(loan_paid_back ~ ., data = train_features_v2)

# Re-calculate VIF
vif_values_v2 <- vif(lm_model_v2)
vif_df_v2 <- data.frame(
  variable = names(vif_values_v2),
  vif = as.numeric(vif_values_v2),
  row.names = NULL
) %>% arrange(desc(vif))
kable(vif_df_v2)
```

As we can see from above table, now VIF for all variables is under 2.

### Data Leakage Check

In this section, we will perform three tests to confirm if there is any data leakage with regards to two variables - **Working Flag** and **Debt to Income Ratio**. 

In first test, we will compare IV using two versions of the variable. If IV collapses drastically under a noisy or lagged version, the original variable leaks.

```{r iv_test, include = TRUE}
# Create function to estimate IV
iv_fun <- function(df, var, target){
  iv(df, y = target, x = var)
}

# Create function to compare IV 
iv_compare <- function(df, var, target){
  original_iv <- iv_fun(df, var, target)

  # Add noise to simulate "pre-origination uncertainity"
  df$noisy <- jitter(df[[var]])
  noisy_iv <- iv_fun(df, "noisy", target)

  return(list(original=original_iv, noisy=noisy_iv))
}

iv_compare(train_features_v2, "debt_to_income_ratio", "loan_paid_back")
iv_compare(train_features_v2, "working_flag_woe", "loan_paid_back")
```

From IV test, it looks like there is a data leakage issue for both variables.

Let's move to second test using correlation.

```{r cor_test}
cor(train_features_v2$debt_to_income_ratio, train_features_v2$loan_paid_back)
cor(train_features_v2$loan_paid_back, train_features_v2$working_flag_woe)
```

In this test, only **Working Flag** failed it as its correlation was greater than 0.5.

In the final test, we will plot WoE bins.

```{r woe_test, include = TRUE}
plot_woe <- function(df, var, target){
  bins <- woebin(df, y = target, x = var)
  woebin_plot(bins)
}
plot_woe(train_features_v2, "debt_to_income_ratio", "loan_paid_back")
plot_woe(train_features_v2, "working_flag_woe", "loan_paid_back")
```

In this test also, **Working Flag** failed the test as it has a perfect monotonic curve shape. 

Given that this is a synthetic dataset on Kaggle, we will be keeping both these variables in our dataset as they have very strong predictive power.

### LASSO

LASSO (Least Absolute Shrinkage and Selection Operator) is a regularized version of logistic or linear regression that performs:

- Variable Selection
- Coefficient Shrinkage
- Model Simplification
- Prevention of Overfitting

It is widely used in credit-risk modeling to identify the strongest predictors before performing logistic regression.

Let's LASSO.

```{r lasso, include = TRUE}
# Create dataset
x <- model.matrix(loan_paid_back ~ ., train_features_v2)[, -1]
y <- train_features_v2$loan_paid_back

# Perform cross-validated LASSO
lasso_cv <- cv.glmnet(
  x, y,
  family = "binomial",
  alpha = 1,
  nfolds = 10
)

# Extract selected variables 
coef_lasso <- coef(lasso_cv, s = "lambda.min")
selected_vars_lasso <- rownames(coef_lasso)[coef_lasso[, 1] != 0]
kable(data.frame(
  variable = selected_vars_lasso[selected_vars_lasso != "(Intercept)"])
  )
```

After performing variable selection using LASSO, only variable to be dropped is **Loan Amount**. It makes business intuitive sense as well because it is a representation of lender's decision and not borrower's quality.

## Feature Engineering

### Logistic Regression

In this section, we will transform final set of variables to make them suitable for Logistic Regression.

First, we will perform of WOE Binning of numeric variables - **DTI**, **Annual Income**, **Interest Rate** and **Credit Score**.

```{r woe, include = TRUE}
# Create dataset
final_vars <- selected_vars_lasso[selected_vars_lasso != "(Intercept)"]
final_vars[8] <- "loan_paid_back"
train_final <- train_features_v2[,..final_vars]
final_vars_test <- final_vars
final_vars_test[8] <- "id"
test_final <- test_features_v2[,..final_vars_test]

# Create Bins
num_bins <- woebin(
  train_final,
  y = "loan_paid_back",
  x = c("debt_to_income_ratio", "credit_score", "annual_income","interest_rate")
)

# Plot WoE Bins
woebin_plot(num_bins)
```

As seen from above plot, we can see that the positive probability has monotonic relationship with all WoE bins of numeric variables. This will be our final transformed variables that we will use for building logistic regression model.

```{r woe_bins}
# Apply WoE Encoding to train dataset
train_final_lr <- woebin_ply(train_final, num_bins)

# Apply WoE Encoding to test dataset
test_final_lr <- woebin_ply(test_final, num_bins)

# Save Datasets
save(train_final_lr, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_final_lr.Rdata")

save(test_final_lr, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_final_lr.Rdata")
```

### Tree-based Models (XGBoost, CatBoost, LightGBM)

We are entering into advanced engineering phase required for tree-based models. We will be leveraging the advanced feature engineering phase as demonstrated by **Chris Deotte** in his [notebook](https://www.kaggle.com/code/cdeotte/feature-engineering-with-rapids-lb-38-847).

We will try to engineer our existing variables to increase our feature space so that tree-based models can use the feature space to identify non-linear relationships that linear models like logistic regression aren't able to capture.

As we have seen in our EDA, there are two variables - **DTI** and **Employment Status** with very high IV. This indicates that this data is synthetically created and we can leverage these two variables to improve our model performance.  

In line with the guidance from Chris's notebook, we will be performing target encoding within the nested folds of our cross-validation based model training workflow. Therefore, we will be doing it in our **Model Development** notebook.

Before going to it, I will combine tree and test datasets and convert all character variables to factor and remove the derived variables - **Grade** and **Working Flag**.

```{r tree_base}
# Combine Train and Test Dataset together
full <- rbind(train, test, fill = TRUE)

# Convert Categorical Variable to Factor Variables
full_tree <- full %>%
                  mutate_if(sapply(full, is.character), as.factor) %>%
                  select(-grade, -working_flag)

# Save Base dataset with raw variables in form of train and test datasets
train_tree <- full_tree %>% filter(!is.na(loan_paid_back))
test_tree <- full_tree %>% filter(is.na(loan_paid_back)) %>% select(-loan_paid_back)
```

In this section, we will apply following three functions that I have created to derive new variables:

- **Binning of numeric variables**: Quantile for skewed continuous variables and fixed binning for normally distributed continuous variables
- **Interaction variables**: Interaction between key numeric and factor variables
- **Aggregated encoding**: Encoding of all variables in original dataset using aggregate statistics like mean and count 

```{r adv_fe}
start_time <- Sys.time()
# Load functions 
source("/workspace/Projects/kaggle-credit-risk-s5e11/helpR_funcs/fe_bigrams.R")
source("/workspace/Projects/kaggle-credit-risk-s5e11/helpR_funcs/fe_round.R")
source("/workspace/Projects/kaggle-credit-risk-s5e11/helpR_funcs/fe_orig.R")

# Set data tables
dt_train <- setDT(train_tree)
dt_test <- setDT(test_tree)

# Define target variable 
target <- "loan_paid_back"

# Define id variable
id <- "id"

# Define non-features
non_features <- list(id, target)

# Define features
features <- names(dt_train[, .SD, .SDcols = setdiff(names(dt_train), non_features)])
numeric <- names(dt_train[, .SD, .SDcols = is.numeric])
factors <- names(dt_train[,.SD, .SDcols = setdiff(names(dt_train), union(numeric, target))])

# Create list of data tables
dt_list = list(dt_train, dt_test)

# Apply bigram function to both train and test 
bigram_features <- setdiff(features, c("annual_income", "loan_amount"))
fe_bigrams(dt_list, cols = bigram_features)

# Apply round function to both train and test 
round_features <- c("annual_income", "loan_amount")
fe_round(dt_list, cols = round_features)

# Apply aggregated encoding using orignial dataset to both train and test
# assume train_dt, test_dt, orig_dt exist in workspace
final_fe <- fe_orig(orig,dt_list,features,target)

# Assign them 
train_fe <- final_fe[[1]]
test_fe <- final_fe[[2]]

# Save datasets
save(train_fe, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_fe.Rdata")

save(test_fe, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_fe.Rdata")
```

```{r comp}


# Compare tables
inter <- colnames(train_fe %>% dplyr::select(starts_with("inter_")))
round <- colnames(train_fe %>% dplyr::select(starts_with("round_")))
orign <- colnames(train_fe %>% dplyr::select(starts_with("orig_")))



kaggle <- fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_preproc_v3.csv")
str(kaggle)
identical(dt_train, kaggle)

#INTER
DT1 <- kaggle[,.SD, .SDcols = inter]
DT2 <- train_fe[,.SD, .SDcols = inter]

compare_cols <- data.table::data.table(
  column = names(DT1),
  class_DT1 = sapply(DT1, class),
  class_DT2 = sapply(DT2, class),
  same_class = sapply(DT1, class) == sapply(DT2, class),
  identical_col = sapply(names(DT1), function(col) all.equal(DT1[[col]], DT2[[col]])),
  unique_DT1 = sapply(DT1, function(x) length(unique(x))),
  unique_DT2 = sapply(DT2, function(x) length(unique(x)))
)

compare_cols

# Find mismatch
find_mismatches <- function(DT1, DT2, col) {
  idx <- DT1[[col]] != DT2[[col]] | (is.na(DT1[[col]]) != is.na(DT2[[col]]))
  mism <- data.table::data.table(
    row = which(idx),
    DT1_value = DT1[[col]][idx],
    DT2_value = DT2[[col]][idx]
  )
  return(mism)
}

mism <- find_mismatches(DT1, DT2, "inter_interest_rate_gender")
head(mism, 20)
nrow(mism)


# ORIG
DT1 <- kaggle[,.SD, .SDcols = orign]
DT2 <- train_fe[,.SD, .SDcols = orign]

compare_cols <- data.table::data.table(
  column = names(DT1),
  class_DT1 = sapply(DT1, class),
  class_DT2 = sapply(DT2, class),
  same_class = sapply(DT1, class) == sapply(DT2, class),
  identical_col = sapply(names(DT1), function(col) all.equal(DT1[[col]], DT2[[col]])),
  unique_DT1 = sapply(DT1, function(x) length(unique(x))),
  unique_DT2 = sapply(DT2, function(x) length(unique(x)))
)

compare_cols

# ROUND
DT1 <- kaggle[,.SD, .SDcols = round]
DT2 <- train_fe[,.SD, .SDcols = round]

compare_cols <- data.table::data.table(
  column = names(DT1),
  class_DT1 = sapply(DT1, class),
  class_DT2 = sapply(DT2, class),
  same_class = sapply(DT1, class) == sapply(DT2, class),
  identical_col = sapply(names(DT1), function(col) all.equal(DT1[[col]], DT2[[col]])),
  unique_DT1 = sapply(DT1, function(x) length(unique(x))),
  unique_DT2 = sapply(DT2, function(x) length(unique(x)))
)
```