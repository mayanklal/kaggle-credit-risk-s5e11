---
title: "Credit Risk Scorecard Model - Feature Engineering"
author: "Mayank Lal"
date: today
format: 
    html:
        code-fold: true
        code-summary: "Code"
        code-overflow: wrap
execute:
  echo: false
  include: false
  warning: false
---
```{r setup}
## Exploratory Data Analysis
library(tidyverse) # Data Manipulation and Visualizaton
library(data.table) # Large datasets
library(vtable) # Formatted Summary Table
library(corrplot) # Correlation
library(patchwork) # Grid of plots
library(scales) # For Percentage Labels
library(rpart) # Simple Decision Tree for interaction analysis
library(rpart.plot) # Plotting Decision Trees

## Variable Selection and Feature Engineering
library(scorecard) # Information Value
library(pROC) # Gini
library(gt) # Formattting Table for publishing
library(knitr) # For simple formatting
library(DT) # For interactive tables
library(stringr) # Selecting sub-strings
library(Hmisc) # Variabel Clustering
library(car) # VIF
library(glmnet) # LASSO
library(janitor) # Cleaning names
library(caret) # Create Folds
```

```{r setup}
set.seed(123)
```
```{r data}
# Load Data
train <- fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/raw/train.csv")
test <- fread("/workspace/Projects/kaggle-credit-risk-s5e11/data/raw/test.csv")
```


```{r common_funcs}
plot_bubble_chart <- function(data, var, target_var, title, x_label) {
bubble <- data %>%
  group_by({{var}}) %>%
  summarise(
    good_rate = mean({{target_var}} == 1),
    count = n()
  ) %>%
  ggplot(aes(x = {{var}}, y = good_rate, size = count)) +
  geom_point(alpha = 0.5) +
  scale_size(range = c(10, 20), name = "No. of Customers") +
  labs(
        title = title,
        x = x_label,
        y = "Loan Repayment Rate"
    ) +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10))
return(bubble)
}
```

## Variable Selection

We begin this journey by selecting variables for model development using regression and tree-based techniques. 

First, we will look at Information Value.

### Information Value

In this section, we will calculate Information Value to filter variables for regression / scorecard modeling. <To be elaborated further>

```{r information_value, include = TRUE}
vars <- colnames(train)[-1]
iv_table <- iv(train[, ..vars], y = "loan_paid_back")
kable(iv_table, caption = "Information Value of Variables")
```

If this wasn't a kaggle competition with a synthetic dataset, we would have only selected variables with IV > 0.02. This would have given us following variables:

- Employment Status
- DTI
- Loan Amount
- Annual Income
- Credit Score
- Grade Sub-grade
- Interest Rate

But since it is a kaggle competition, we would not be enforcing such a hard criterion for filtering variables. We will combine IV with our business intuition to select following variables:

- Employment Status
- DTI
- Loan Amount
- Annual Income
- Credit Score
- Grade Sub-grade
- Interest Rate
- **Loan Purpose**
- **Education Level**

Why have we added **Loan Purpose** and **Education Level**? We have added them back based on our EDA analysis in which we saw a clear and noticeable difference in loan repayment rate between different classes of these variables. Also, they make business intuitive sense as well. 

**Loan Purpose** will help us understand if a loan taken for a given purpose has higher likelihood of repayment or not.  **Education Level** can be an indicator of future employability and income of the customer reducing the likelihood of loan default. Also, there might be an interaction between annual income and education level. We will investigate this in later phase.

Also, one thing to note here is that IV of Employment Status and DTI is very suspiciously high (>0.5). An IV this high is rare in real-world credit datasets. It signals:

- Possibility of data leakage
- Variable may be directly encoding the target
- Or near-direct proxy for “good/bad”

But since this is a synthetic dataset on Kaggle, it can be expected. For sanctity sake, we will also perform data leakage check later on.

### Clustering-based Filtering

In this section, we will perform process to filter variables using **Hierarchical Clustering**.

Before performing clustering, we will have to transform categorical variables to numerical variables. After IV-based filtering, there are four categorical variables: **Employment Status**, **Grade Sub-grade**, **Education Level** and **Loan Purpose**

First let's transform **Employment Status**. As we have seen in our EDA, the loan repayment rate is very high for three classes - **Retired, Employed and Self-employed**. Similarly, it is ver low for remaining two classes - **Student and Unemployed**. Given the clear separation, we can merge them into two groups and then apply **WOE encoding** to ensure that we don't lose any information.

```{r employment_status, include = TRUE}
# Create Working Flag
train$working_flag <- ifelse(train$employment_status %in% c("Retired", "Employed", "Self-employed"),1 ,0)
test$working_flag <- ifelse(test$employment_status %in% c("Retired", "Employed", "Self-employed"),1 ,0)

# Apply WOE Encoding
es_bins <- woebin(
  train,
  y = "loan_paid_back",
  x = "working_flag"
)

# Plot WOE Bins of Employment Status/Working Flag
woebin_plot(es_bins)
```

Now, we will transform **Grade Sub-grade** variable. As we have seen in our EDA analysis, they have monotonic relationship with loan repayment rate. Also, it is an ordinal variable with clear order: **A>B>C>D>E>F**. Therefore, we will try to merge the sub-grades into one grade and then convert into an ordinal factor variable. Then finally into a numeric variable.

After that during feature engineering phase, we will try to find which transformation or encoding will be suitable for it - **WOE Encoding** or **Target Encoding**.

```{r grade, include = TRUE}
# Merge Sub-grades to Grades using ordinal factor conversion
train$grade <- str_sub(train$grade_subgrade, 1, 1)
train$grade <- factor(
  train$grade,
  levels = c("A", "B", "C", "D", "E", "F"),
  labels = c(6,5,4,3,2,1),
  ordered = FALSE
)

train$grade <- as.numeric(train$grade)


test$grade <- str_sub(test$grade_subgrade, 1, 1)
test$grade <- factor(
  test$grade,
  levels = c("A", "B", "C", "D", "E", "F"),
  ordered = TRUE
)

test$grade <- as.numeric(test$grade)

# Plot Bubble plot of Grade
#| fig.width: 12
#| fig.height: 8
#| fig.align: "center"
plot_bubble_chart(train, grade, loan_paid_back, "Bubble Plot of Grade by Loan Repayment Rate", "Grade")
```
As we can see from above chart, the Sub-grades have been merged to their Grade and converted to numeric.

Next we will transform both **Education Level** and **Loan Purpose**. Both of these variables have multiple classes and don't have definite order. There is slight variation in loan repayment rate of each class of these variables. Like previously we have done with **Employment Status**, we would be applying **WOE encoding** to convert them to numeric while preserving the signal embedded in them.

```{r education_loan_purpose, include = TRUE}
# Apply WOE Encoding
ed_lp_bins <- woebin(
  train,
  y = "loan_paid_back",
  x = c("education_level", "loan_purpose")
)

# Plot WOE Bins of Education Level and Loan Purpose
woebin_plot(ed_lp_bins)
```

From the above charts, we can see that we are able to maintain monotonic relationship through **WOE Encoding**. It automatically merges classes within a variable to attain that. This will be used while performing modeling using logistic regression as well. Now, we will perform **Hierarchical Clustering**.

```{r hc, include = TRUE}
# Filter variables based on IV statistic and Business Intuition
train_corr <- train %>% select(-employment_status, 
                               -grade_subgrade,
                               -gender,
                               -marital_status)

test_corr <- test %>% select(-employment_status, 
                               -grade_subgrade,
                               -gender,
                               -marital_status)

# Perform WoE Binning to Education Level, Loan Purpose and Working Flag
bins <- woebin(
  train_corr,
  y = "loan_paid_back",
  x = c("education_level", "loan_purpose", "working_flag")
)

# Apply WoE Encoding to Education Level, Loan Purpose and Working Flag
train_corr <- woebin_ply(train_corr, bins)
test_corr <- woebin_ply(test_corr, bins)


# Create correlation matrix
corr_matrix <- abs(cor(train_corr[, c(-1, -7)], use = "pairwise.complete.obs"))

# Create distance matrix
dist_matrix <- as.dist(1 - corr_matrix)

# Perform hierarchical clustering
hc <- hclust(dist_matrix, method = "complete")

# Plot Dendogram 
plot(hc, main = "Variable Clustering Dendogram")
```

From the above dendogram plot, we can see that there are **two key clutsters** as shown below:

- **Credit Score**, **Grade**, and **Interest Rate**
- **Debt to Income Ratio** and **Working Flag**

From each of these clusters, we can select one variable with highest IV value. But before doing that we can also check their correlation and our business intuition. Based on both of them, we will decide which variables need to be kept for further processing and feature engineering.

We will recalculate Information Value (IV) to determine if we need to make a decision about filtering out any variables from each of the above clusters.

```{r hc_vars, include=TRUE}
# Cut tree at correlation threshold of 0.75
clusters <- cutree(hc, h = 0.75)

# Recompute IV
iv_table_transformed <- iv(train_corr[,-1], y = "loan_paid_back")

# Build cluster table with IV 
cluster_df <- data.frame(variable = colnames(train_corr[,c(-1,-7)]),
                         cluster = clusters) %>%
                         left_join(iv_table_transformed, by = "variable") %>%
                         arrange(cluster, desc(info_value))
kable(cluster_df)
```

Based on above table, we will keep all the variables for now. Though we should keep in mind that we can eliminate two variables from Cluster 3 if required later on.

### Multicollinearity

In this section, we will perform filtering using **Variance Inflation Factor (VIF)** statistic. VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity between predictors.

If a variable is strongly correlated with other predictors, its coefficient becomes unstable and its standard error becomes larger.
VIF quantifies this effect. <Add Mathematical Formula as well>

```{r vif, include = TRUE}
# Define target, id, and feature variables
target_var <- "loan_paid_back"
id_var <- "id"
features_train <- train_corr[, !(names(train_corr) %in% c(id_var))]

# Keep only features
train_features <- train_corr[,..features_train]

# Estimate VIF using simple linear regression model
lm_model <- lm(loan_paid_back ~ ., data = train_features)

# Calculate VIF
vif_values <- vif(lm_model)
vif_df <- data.frame(
  variable = names(vif_values),
  vif = as.numeric(vif_values),
  row.names = NULL
) %>% arrange(desc(vif))
kable(vif_df)
```

Based on above table, we can finally make the decision to drop **Grade** due to multicollinearity (VIF > 5) with **Credit Score** and **Interest Rate**. Also, Grade can be considered as discretized version of credit score which adds no information to our model. Therefore, we will be dropping **Grade** from our model.

Let's recalculate VIF after dropping **Grade**.

```{r vif2, include = TRUE}
# Drop Grade
features_train_v2 <- train_features[, !(names(train_features) %in% c("grade"))]
features_test_v2 <- test_corr[, !(names(test_corr) %in% c("grade"))]

# Keep only features
train_features_v2 <- train_features[,..features_train_v2]
test_features_v2 <- test_corr[,..features_test_v2]

# Estimate VIF using simple linear regression model
lm_model_v2 <- lm(loan_paid_back ~ ., data = train_features_v2)

# Re-calculate VIF
vif_values_v2 <- vif(lm_model_v2)
vif_df_v2 <- data.frame(
  variable = names(vif_values_v2),
  vif = as.numeric(vif_values_v2),
  row.names = NULL
) %>% arrange(desc(vif))
kable(vif_df_v2)
```

As we can see from above table, now VIF for all variables is under 2.

### Data Leakage Check

In this section, we will perform three tests to confirm if there is any data leakage with regards to two variables - **Working Flag** and **Debt to Income Ratio**. 

In first test, we will compare IV using two versions of the variable. If IV collapses drastically under a noisy or lagged version, the original variable leaks.

```{r iv_test, include = TRUE}
# Create function to estimate IV
iv_fun <- function(df, var, target){
  iv(df, y = target, x = var)
}

# Create function to compare IV 
iv_compare <- function(df, var, target){
  original_iv <- iv_fun(df, var, target)

  # Add noise to simulate "pre-origination uncertainity"
  df$noisy <- jitter(df[[var]])
  noisy_iv <- iv_fun(df, "noisy", target)

  return(list(original=original_iv, noisy=noisy_iv))
}

iv_compare(train_features_v2, "debt_to_income_ratio", "loan_paid_back")
iv_compare(train_features_v2, "working_flag_woe", "loan_paid_back")
```

From IV test, it looks like there is a data leakage issue for both variables.

Let's move to second test using correlation.

```{r cor_test}
cor(train_features_v2$debt_to_income_ratio, train_features_v2$loan_paid_back)
cor(train_features_v2$loan_paid_back, train_features_v2$working_flag_woe)
```

In this test, only **Working Flag** failed it as its correlation was greater than 0.5.

In the final test, we will plot WoE bins.

```{r woe_test, include = TRUE}
plot_woe <- function(df, var, target){
  bins <- woebin(df, y = target, x = var)
  woebin_plot(bins)
}
plot_woe(train_features_v2, "debt_to_income_ratio", "loan_paid_back")
plot_woe(train_features_v2, "working_flag_woe", "loan_paid_back")
```

In this test also, **Working Flag** failed the test as it has a perfect monotonic curve shape. 

Given that this is a synthetic dataset on Kaggle, we will be keeping both these variables in our dataset as they have very strong predictive power.

### LASSO

LASSO (Least Absolute Shrinkage and Selection Operator) is a regularized version of logistic or linear regression that performs:

- Variable Selection
- Coefficient Shrinkage
- Model Simplification
- Prevention of Overfitting

It is widely used in credit-risk modeling to identify the strongest predictors before performing logistic regression.

Let's LASSO.

```{r lasso, include = TRUE}
# Create dataset
x <- model.matrix(loan_paid_back ~ ., train_features_v2)[, -1]
y <- train_features_v2$loan_paid_back

# Perform cross-validated LASSO
lasso_cv <- cv.glmnet(
  x, y,
  family = "binomial",
  alpha = 1,
  nfolds = 10
)

# Extract selected variables 
coef_lasso <- coef(lasso_cv, s = "lambda.min")
selected_vars_lasso <- rownames(coef_lasso)[coef_lasso[, 1] != 0]
kable(data.frame(
  variable = selected_vars_lasso[selected_vars_lasso != "(Intercept)"])
  )
```

After performing variable selection using LASSO, only variable to be dropped is **Loan Amount**. It makes business intuitive sense as well because it is a representation of lender's decision and not borrower's quality.

## Feature Engineering

### Logistic Regression

In this section, we will transform final set of variables to make them suitable for Logistic Regression.

First, we will perform of WOE Binning of numeric variables - **DTI**, **Annual Income**, **Interest Rate** and **Credit Score**.

```{r woe, include = TRUE}
# Create dataset
final_vars <- selected_vars_lasso[selected_vars_lasso != "(Intercept)"]
final_vars[8] <- "loan_paid_back"
train_final <- train_features_v2[,..final_vars]
final_vars_test <- final_vars
final_vars_test[8] <- "id"
test_final <- test_features_v2[,..final_vars_test]

# Create Bins
num_bins <- woebin(
  train_final,
  y = "loan_paid_back",
  x = c("debt_to_income_ratio", "credit_score", "annual_income","interest_rate")
)

# Plot WoE Bins
woebin_plot(num_bins)
```

As seen from above plot, we can see that the positive probability has monotonic relationship with all WoE bins of numeric variables. This will be our final transformed variables that we will use for building logistic regression model.

```{r woe_bins}
# Apply WoE Encoding to train dataset
train_final_lr <- woebin_ply(train_final, num_bins)

# Apply WoE Encoding to test dataset
test_final_lr <- woebin_ply(test_final, num_bins)

# Save Datasets
save(train_final_lr, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_final_lr.Rdata")

save(test_final_lr, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_final_lr.Rdata")
```

### Tree-based Models (XGBoost, CatBoost, LightGBM)

Based on our variable selection process performed for logistic regression, we have a follwing set of **candidate variables**:

- Debt to Income Ratio (DTI)
- Credit Score
- Interest Rate
- Annual Income
- Employment Status
- Education Level
- Grade Sub-grade
- Loan Purpose

We will try to engineer these variables to increase our feature space so that tree-based models can use the feature space to identify non-linear relationships that linear models like logistic regression aren't able to capture.

First, we will be focusing on numeric variables. We will perform **quantile binning** to create binned variables. These **binned variables** will then be target encoded during k-fold cross validation. Also, we will be supplying numeric variables in three forms - raw, binned, and target encoded - in an incremental way. We are following this strategy so that XGBoost can identify which form of variables captures most useful signal and utilize that while building the model.

```{r tree_num_vars}
# Select Candidate Variables
train_tree <- train %>% select(
                        id,
                        loan_paid_back,
                        debt_to_income_ratio,
                        credit_score,
                        interest_rate,
                        annual_income,
                        employment_status,
                        education_level,
                        grade_subgrade,
                        loan_purpose,
                        grade,
                        working_flag
                        )

test_tree <- test %>% select(
                        id,
                        debt_to_income_ratio,
                        credit_score,
                        interest_rate,
                        annual_income,
                        employment_status,
                        education_level,
                        grade_subgrade,
                        loan_purpose,
                        grade,
                        working_flag
                        )

# Function to create bins
create_bins <- function(train_vec, test_vec, n_bins = 5) {

  # compute breaks on train
  breaks <- quantile(train_vec,
                     probs = seq(0, 1, length.out = n_bins + 1),
                     na.rm = TRUE,
                     include.lowest = TRUE)

  # bin train & test
  train_bin <- cut(train_vec, breaks = breaks, include.lowest = TRUE)
  test_bin  <- cut(test_vec,  breaks = breaks, include.lowest = TRUE)

  return(list(train_bin = train_bin,
              test_bin  = test_bin,
              breaks    = breaks))
}

# Bin Numeric Variables
vars_to_bin <- c("debt_to_income_ratio", "credit_score", 
                 "annual_income")

train_tree_binned <- train_tree
test_tree_binned <- test_tree

for (v in vars_to_bin) {
  res <- create_bins(train_tree_binned[[v]], test_tree_binned[[v]], n_bins = 5)
  train_tree_binned[[paste0(v, "_bin")]] <- res$train_bin
  test_tree_binned[[paste0(v, "_bin")]]  <- res$test_bin
}
```

Next, we will collapse categories of **Loan Purpose** and **Education Level** like we did for **Employment Status** and **Grade Sub-grade**.
We will also convert these categorical variables to **factor variables**. These **factor variables** will be then **one hot encoded** and **target encoded** during k-fold cross validation. Also, we will be supplying factor variables in three forms - raw, one hot encoded, and target encoded - in an incremental way. The strategy remains the same as before followed in numeric variables.


```{r tree_factors}
# Simplify Categorical Variables
train_tree_factors <-  train_tree_binned %>%
                  mutate(
                  loan_purpose_main = case_when(
                    loan_purpose == "Business" ~ "Business",
                    loan_purpose == "Home" ~ "Home",
                    loan_purpose == "Debt consolidation" ~ "Debt consolidation",
                    loan_purpose %in% c("Education", "Medical") ~ "Education & Medical",
                    TRUE ~ "Other Purposes"
                  ),
                  education_level_main = case_when(
                    education_level == "Bachelor's" ~ "Bachelor's",
                    education_level == "High School" ~ "High School",
                    education_level == "PhD" ~ "PhD",
                    TRUE ~ "Others"
                  )
                )

test_tree_factors <-  test_tree_binned %>%
                  mutate(
                  loan_purpose_main = case_when(
                    loan_purpose == "Business" ~ "Business",
                    loan_purpose == "Home" ~ "Home",
                    loan_purpose == "Debt consolidation" ~ "Debt consolidation",
                    loan_purpose %in% c("Education", "Medical") ~ "Education & Medical",
                    TRUE ~ "Other Purposes"
                  ),
                  education_level_main = case_when(
                    education_level == "Bachelor's" ~ "Bachelor's",
                    education_level == "High School" ~ "High School",
                    education_level == "PhD" ~ "PhD",
                    TRUE ~ "Others"
                  )
                )

# Convert to factors               
train_tree_factors <- train_tree_factors %>%
                mutate_if(sapply(train_tree_factors, is.character), as.factor)

test_tree_factors <- test_tree_factors %>%
                mutate_if(sapply(test_tree_factors, is.character), as.factor)

# Convert Working Flag to factors as well
train_tree_factors$working_flag <- factor(train_tree_factors$working_flag, levels = c(0,1))

test_tree_factors$working_flag <- factor(test_tree_factors$working_flag, levels = c(0,1))

# Create factor form of Grade variable as well
train_tree_factors$grade_factor <- factor(train_tree_factors$grade)

test_tree_factors$grade_factor <- factor(test_tree_factors$grade)
```

Finally, we will be creating following interaction variables based on business intuition and domain knowledge:

- DTI x Credit Score
- DTI x Employment Status
- DTI x Annual Income
- DTI x Loan Purpose
- Interest Rate x Credit Score
- Annual Income x Education Level
- Employment Status x Annual Income
- Employment Status x Credit Score

```{r interaction_vars}
# Interaction between Numeric Variables 
train_tree_num_intr <- train_tree_factors %>% mutate(
                                                    dti_cs = debt_to_income_ratio*credit_score,
                                                    dti_annual_income = debt_to_income_ratio*annual_income,
                                                    dti_es = debt_to_income_ratio*as.numeric(working_flag),
                                                    ir_cs = interest_rate*credit_score,
                                                    es_annual_income = as.numeric(working_flag)*annual_income,
                                                    es_cs = as.numeric(working_flag)*credit_score
                                                    )

test_tree_num_intr <- test_tree_factors %>% mutate(
                                                    dti_cs = debt_to_income_ratio*credit_score,
                                                    dti_annual_income = debt_to_income_ratio*annual_income,
                                                    dti_es = debt_to_income_ratio*as.numeric(working_flag),
                                                    ir_cs = interest_rate*credit_score,
                                                    es_annual_income = as.numeric(working_flag)*annual_income,
                                                    es_cs = as.numeric(working_flag)*credit_score
                                                    )
# Interaction between Factor and Numeric Variables 
# DTI x Loan Purpose
loan_purpose_dummies <- model.matrix(~ loan_purpose_main - 1, data = train_tree_num_intr) %>% as.data.frame() %>% as_tibble()

dti_loan_purpose <- loan_purpose_dummies*train_tree_num_intr$debt_to_income_ratio

loan_purpose_dummies_test <- model.matrix(~ loan_purpose_main - 1, data = test_tree_num_intr) %>% as.data.frame() %>% as_tibble()

dti_loan_purpose_test <- loan_purpose_dummies_test*test_tree_num_intr$debt_to_income_ratio

# Annual Income x Education Level
education_lvl_dummies <- model.matrix(~ education_level_main - 1, data = train_tree_num_intr) %>% as.data.frame() %>% as_tibble()

ed_lvl_income <- education_lvl_dummies*train_tree_num_intr$annual_income

education_lvl_dummies_test <- model.matrix(~ education_level_main - 1, data = test_tree_num_intr) %>% as.data.frame() %>% as_tibble()

ed_lvl_income_test <- education_lvl_dummies_test*test_tree_num_intr$annual_income

# Bind all in one final data frame
train_tree_final <- train_tree_num_intr %>%
                    bind_cols(dti_loan_purpose) %>%
                    bind_cols(ed_lvl_income)

test_tree_final <- test_tree_num_intr %>%
                    bind_cols(dti_loan_purpose_test) %>%
                    bind_cols(ed_lvl_income_test)
```

We are entering now engineering or leaderboard hacking phase. I need a better understanding of this. For now, I am trying to implement it on top of my existing feature space to see what magic happens with it.

```{r bin_interactions}
# Create function to apply interactions with binned variables
encode_int <- function(x) {
  as.integer(factor(x))
}

add_bin_interaction <- function(df, bin_col, cat_col) {
  df <- df %>% as_tibble()
  
  # Convert to integer encoding (robust for tree models)
  bin_int <- encode_int(df[[bin_col]])
  cat_int <- encode_int(df[[cat_col]])
  
  # New interaction column name
  new_col <- paste0(bin_col, "_x_", cat_col)

  df[[new_col]] <- bin_int * cat_int

  return(df)
}

interactions <- list(
  c("credit_score_bin", "working_flag"),
  c("credit_score_bin", "loan_purpose_main"),
  c("debt_to_income_ratio_bin", "working_flag"),
  c("debt_to_income_ratio_bin", "loan_purpose_main"),
  c("annual_income_bin", "education_level_main")
)

# apply to train
for (pair in interactions) {
  train_tree_final <- add_bin_interaction(train_tree_final, pair[1], pair[2])
}

# apply to test
for (pair in interactions) {
  test_tree_final <- add_bin_interaction(test_tree_final, pair[1], pair[2])
}
````
```{r count_encoding}
# Create function for count encoding
count_encode <- function(df, col) {
  col_quo <- enquo(col)
  col_nm  <- quo_name(col_quo)

  df %>%
    add_count(!!col_quo, name = paste0(col_nm, "_count"))
}

# Apply count encoding to train and test datasets
train_count_enc <- train_tree_final %>%
            count_encode(loan_purpose_main) %>%
            count_encode(education_level_main) %>%
            count_encode(grade_factor) %>%
            count_encode(debt_to_income_ratio_bin) %>%
            count_encode(credit_score_bin) %>%
            count_encode(annual_income_bin)

test_count_enc <- test_tree_final %>%
            count_encode(loan_purpose_main) %>%
            count_encode(education_level_main) %>%
            count_encode(grade_factor) %>%
            count_encode(debt_to_income_ratio_bin) %>%
            count_encode(credit_score_bin) %>%
            count_encode(annual_income_bin)
```

```{r group_stats}
# Create function to add group statistics of selected columns
add_group_stats <- function(df, group_col, value_col){
  group_col <- rlang::sym(group_col)
  value_col <- rlang::sym(value_col)

  stats_df <- df %>%
              group_by(!!group_col) %>%
              summarise(
                mean = mean(!!value_col, na.rm = TRUE),
                median = median(!!value_col, na.rm = TRUE),
                sd = sd(!!value_col, na.rm = TRUE),
                .groups = "drop"
              ) %>%
              rename(
                !!paste0(group_col,"_", value_col, "_mean") := mean,
                !!paste0(group_col,"_", value_col, "_median") := median,
                !!paste0(group_col,"_", value_col, "_sd") := sd
              )
              df %>% left_join(stats_df, by = rlang::as_string(group_col))
}

# Apply group stats function to train and test dataset
train_group_stats <- train_count_enc %>% add_group_stats("loan_purpose_main", "debt_to_income_ratio") %>%
add_group_stats("loan_purpose_main", "credit_score") %>%
add_group_stats("loan_purpose_main", "interest_rate") %>%
add_group_stats("loan_purpose_main", "annual_income")

test_group_stats <- test_count_enc %>% add_group_stats("loan_purpose_main", "debt_to_income_ratio") %>%
add_group_stats("loan_purpose_main", "credit_score") %>%
add_group_stats("loan_purpose_main", "interest_rate") %>%
add_group_stats("loan_purpose_main", "annual_income")
```

```{r oof_target_encoding}
# Create Folds
folds <- createFolds(train_group_stats$loan_paid_back, k = 5)

# Create function to apply OOF Target Encoding to train and test datasets
oof_target_encode <- function(train, test, col, target, folds){

  train <- as_tibble(train)
  test  <- as_tibble(test)

  col_sym <- rlang::sym(col)
  target_sym <- rlang::sym(target)

  global_mean <- mean(train[[target]], na.rm = TRUE)
  oof <- rep(NA_real_, nrow(train))

  # ----------------------
  # PART 1 — TRAIN OOF TE
  # ----------------------
  for (i in seq_along(folds)){
    val_idx <- folds[[i]]
    train_idx <- setdiff(seq_len(nrow(train)), val_idx)

    te_map <- train[train_idx, ] %>%
      group_by(!!col_sym) %>%
      summarise(te = mean(!!target_sym), .groups = "drop")

    tmp <- train[val_idx, ] %>%
      left_join(te_map, by = col)

    tmp$te[is.na(tmp$te)] <- global_mean

    oof[val_idx] <- tmp$te
  }

  # Add new OOF TE column (keeps all previous columns)
  te_col <- paste0(col, "_te")
  train[[te_col]] <- oof

  # ----------------------
  # PART 2 — TEST FULL TE
  # ----------------------
  full_map <- train %>%
    group_by(!!col_sym) %>%
    summarise(te = mean(!!target_sym), .groups = "drop")

  test <- test %>%
    left_join(full_map, by = col)

  # Rename 'te' to correct name
  test <- test %>% rename(!!te_col := te)

  # Replace NA values
  test[[te_col]][is.na(test[[te_col]])] <- global_mean
  
  # ----------------------
  # Crucial: return FULL train, not a new one
  # ----------------------
  return(list(train_te = train, test_te = test))
}


# Apply OOF TE Encoding 
te_vars <- c("loan_purpose_main", "education_level_main", "grade_factor", "debt_to_income_ratio_bin", "credit_score_bin", "annual_income_bin")

train_te <- train_group_stats
test_te <- test_group_stats

for (v in te_vars){
  res <- oof_target_encode(train_te, test_te, v, "loan_paid_back", folds)
  train_te <- res$train_te
  test_te <- res$test_te
}
```

```{r save_tree_datasets}
# Clean Column Names
train_tree_final <- clean_names(train_tree_final)
test_tree_final  <- clean_names(test_tree_final)

# Clean Column Names of Target Encoded datasets
train_tree_final_te <- as.data.table(clean_names(train_te))
test_tree_final_te  <- as.data.table(clean_names(test_te))


# Save Datasets
save(train_tree_final, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_tree_final.Rdata")

save(test_tree_final, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_tree_final.Rdata")

save(train_tree_final_te, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/train_tree_final_te.Rdata")

save(test_tree_final_te, file = "/workspace/Projects/kaggle-credit-risk-s5e11/data/processed/test_tree_final_te.Rdata")
```


